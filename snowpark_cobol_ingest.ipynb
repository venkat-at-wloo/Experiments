{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9f75fb-4470-4d4b-9995-19183fa1e882",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parsing Cobol files using Snowpark\n",
    "\n",
    "Here is a quick gist for parsing cobol file (dcm), which is present in an external stage (ex: S3).\n",
    "\n",
    "## Solution overview\n",
    "Snowpark can read files from the stage, [doc](https://docs.snowflake.com/en/LIMITEDACCESS/snowpark-python.html#working-with-files-in-a-stage).It is also easy to read cobol file using coboljsonifier, [doc](https://pypi.org/project/coboljsonifier/).\n",
    "With this mind, I wanted to see what would it take Snowpark to read the cobol file, which is stored in an external stage and store the records into a Snowflake table.\n",
    "\n",
    "The cobol file is read and its returned as JSON format. \n",
    "\n",
    "**Pre-requisite**\n",
    " - the external stage should be enabled as a directory table.\n",
    " \n",
    "### Code Logic\n",
    " - Connect to Snowflake\n",
    " - Retreive the list of dicom files hosted in the external stage.\n",
    " - Import each cobol copybook/data file into session\n",
    " - Import python additional libraries into session\n",
    " - Create the UDF, which would parse the dicom file.\n",
    " - Iterate the stage directory table, but invoking the above defined udf.\n",
    " - If successful add the data into a staging table.\n",
    "\n",
    "### Hurdles\n",
    "Before rushing into the code, the steps was not simple as originally thought. The following are some hurdles that I had to overcome, to make the solution work.\n",
    "#### Coboljsonifier library\n",
    "The coboljsonifier library is not part of Snowflake python packages. Hence it needs to \n",
    "be imported dynamically. To overcome this \n",
    " - we upload the library in the external stage\n",
    " - at runtime, we extract the libraries into a temporary folder\n",
    " - we add the folder to system path, this allows the python classes to be imported.\n",
    "\n",
    "#### Staging cobol definition and data file\n",
    "Currently python udf, cannot read files from the stage unless it is part of the import. To overcome this, I iterate through the stage folder where the dcm files are present and add each file as an import. \n",
    "\n",
    "## Limitations \n",
    " - Please read the coboljsonifier [docs](https://github.com/jrperin/cobol-copybook.jsonifier), understand the process.\n",
    " - There could be some limitations on the number of files that can be imported and or total combined size of files.\n",
    " - This code is a temporary hack till python file stream access is in place.\n",
    " - The number of records in the data file can cause issue. particularly worried about the 16mb limit. In this a different strategy might need to done, which i can discuss.\n",
    "\n",
    "## Thoughts\n",
    "What I am demonstrating here is the possibility of parsing cobol files using Python. Hence you would need to\n",
    "fine tune or re-implement the parsing logic, as you see fit. Also I am not demonstrating the process of building the\n",
    "cobol definition file, for this please refer to coboljsonifier doc or the example [examples/README.md](https://raw.githubusercontent.com/jrperin/cobol-copybook.jsonifier/master/examples/README.md).\n",
    "\n",
    "let me know how if this helps out, it will be great!!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b749be1d-fd1e-4217-9140-863b923b2f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Snowpark session\n",
    "import os ,json\n",
    "import dotenv \n",
    "import pandas as pd\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "#Load the login information from env file\n",
    "dotenv.load_dotenv('./sflk.env')\n",
    "\n",
    "#Create a snowpark session\n",
    "connection_parameters = {\n",
    "  \"account\": os.getenv('DEMO_ACCOUNT'),\n",
    "  \"user\": os.getenv('DEMO_USER'),\n",
    "  \"password\": os.getenv('DEMO_PWD'),\n",
    "  \"role\": \"sysadmin\",\n",
    "  \"warehouse\": os.getenv('DEMO_WH'),\n",
    "  \"database\": os.getenv('DEMO_DB'),\n",
    "  \"schema\": os.getenv('DEMO_SCH')\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "#print(session.sql(\"select current_account() ,current_warehouse(), current_database(), current_schema()\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b2fa9b-1662-44b0-9a59-025da28a9afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@stg_hl7data/datasets/cobol/ASCII_BOOK.cob',\n",
       " '@stg_hl7data/datasets/cobol/ASCII_DATA.bin',\n",
       " '@stg_hl7data/datasets/cobol/EBCDIC_BOOK.cob',\n",
       " '@stg_hl7data/datasets/cobol/EBCDIC_DATA.bin',\n",
       " '@stg_hl7data/pyfn_lib/coboljsonifier-1.0.3.tar.gz',\n",
       " '@stg_hl7data/pyfn_lib/simplejson-3.17.6.tar.gz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Iterate the stage 'stg_hl7data', specifically the folder 'datasets/cobol' and retrieve the list of files\n",
    "stage_name = '@stg_hl7data'\n",
    "\n",
    "session.sql(f'''alter stage stg_hl7data refresh;''').collect()\n",
    "\n",
    "data_files = session.sql(f'''\n",
    "    select \n",
    "        concat('{stage_name}/' ,relative_path) as full_path\n",
    "    from directory({stage_name} )\n",
    "    where relative_path like 'datasets/cobol/%';\n",
    "''').collect()\n",
    "\n",
    "# Clear any previous imports\n",
    "session.clear_imports()\n",
    "\n",
    "# Import each file into the session\n",
    "for data_fl_row in data_files:\n",
    "    fl_path = f'{data_fl_row[0]}'\n",
    "    session.add_import(fl_path)\n",
    "    \n",
    "# Add the required libraries\n",
    "libs_to_extract = ['coboljsonifier-1.0.3.tar.gz' ,'simplejson-3.17.6.tar.gz']\n",
    "for lib in libs_to_extract:\n",
    "    session.add_import(f'{stage_name}/pyfn_lib/{lib}')\n",
    "\n",
    "# List out the imports, for debugging purposes.\n",
    "session.get_imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f47b5130-b2d2-4644-86c1-b11091d3c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Define the udf, which would parse the cobol file\n",
    "#\n",
    "\n",
    "from snowflake.snowpark.udf import *\n",
    "from snowflake.snowpark.types import Variant;\n",
    "from snowflake.snowpark.functions import *\n",
    "from snowflake.snowpark import *\n",
    "\n",
    "\n",
    "'''\n",
    "The UDF has the following paramter:\n",
    " - p_copybook_def_path : the full path to the copybook structure file. ex: @stg_hl7data/datasets/cobol/ASCII_BOOK.cob\n",
    " - p_coboldata_path : the full path to the cobol data file. ex: @stg_hl7data/datasets/cobol/ASCII_DATA.bin\n",
    "'''\n",
    "@udf(session=session ,name=\"cobolparser_snowpy\" ,replace=True)\n",
    "def cobolparser_snowpy(p_copybook_def_path: str ,p_coboldata_path: str) -> Variant:\n",
    "    import os ,sys ,json ,tarfile\n",
    "    import importlib.util\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Extract the third party libraries into tmp folder and dynamically import\n",
    "    IMPORT_DIR = sys._xoptions[\"snowflake_import_directory\"]\n",
    "    TARGET_FOLDER = f'/tmp/cobolparser_snowpy' + str(os.getpid())    \n",
    "    libs_to_extract = ['coboljsonifier-1.0.3.tar.gz' ,'simplejson-3.17.6.tar.gz']\n",
    "    for lib in libs_to_extract:\n",
    "        PACKAGE_FNAME = lib.replace('.tar.gz', '')\n",
    "        TARGET_LIB_PATH = f'{TARGET_FOLDER}/{PACKAGE_FNAME}/'\n",
    "        Path(f'{TARGET_LIB_PATH}').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        #detect if the library is tar archived or zip archived\n",
    "        #and extract accordingly\n",
    "        if ('.tar.gz' in lib):\n",
    "            tf = tarfile.open(f'{IMPORT_DIR}{lib}')\n",
    "            tf.extractall(f'{TARGET_FOLDER}')\n",
    "            \n",
    "        elif ('.zip' in lib):\n",
    "            with zipfile.ZipFile(f'{IMPORT_DIR}{lib}', 'r') as zip_ref:\n",
    "                zip_ref.extractall(TARGET_FOLDER)\n",
    "    \n",
    "        #Add the extracted folder to sys path\n",
    "        sys.path.insert(0 ,TARGET_LIB_PATH )\n",
    "        \n",
    "        #nasty coboljsonifier code organization.\n",
    "        if ('coboljsonifier' in lib):\n",
    "            sys.path.insert(0 ,f'{TARGET_LIB_PATH}/src' )\n",
    "        \n",
    "    #Import should be done, only after inserting the target_lib_path into the path\n",
    "    import simplejson\n",
    "    from coboljsonifier.copybookextractor import CopybookExtractor\n",
    "    from coboljsonifier.parser import Parser\n",
    "    from coboljsonifier.config.parser_type_enum import ParseType\n",
    "\n",
    "    # an utility method to list the content of the \n",
    "    # a directory. meant for debugging needs. for ex: to find the\n",
    "    # list of files in the import directory\n",
    "    def list_directory(p_dir):\n",
    "        fls = []\n",
    "        for root, dirs, files in os.walk(p_dir):\n",
    "            for file in files:\n",
    "                fls.append(f'{root}{dirs}{file}')\n",
    "        \n",
    "        dat = {}\n",
    "        dat['files'] = fls\n",
    "        return json.dumps(dat)\n",
    "\n",
    "    # The udf to handle the parsing\n",
    "    def udf(p_copybook_def_path ,p_coboldata_path):\n",
    "        #TODO wrap this in a try/catch for better code\n",
    "        \n",
    "        #Warning :\n",
    "        #The code demoed here is based of 'examples/ascii_parser_test.py' from the\n",
    "        #coboljsonifier project. You might need to modify the functionality based \n",
    "        #of what you are seeing in the field.\n",
    "        copybook_def_full_path = f'{IMPORT_DIR}{p_copybook_def_path}'\n",
    "        coboldata_full_path = f'{IMPORT_DIR}{p_coboldata_path}'\n",
    "        \n",
    "        # Build the Parser\n",
    "        dict_structure = CopybookExtractor(copybook_def_full_path).dict_book_structure\n",
    "        parser = Parser(dict_structure,  ParseType.FLAT_ASCII).build()\n",
    "        \n",
    "        #parse the records\n",
    "        # Warnings : the no of records could seriously affect, you might need\n",
    "        # to look into adopting a slightly different pattern of processing the\n",
    "        # data, based of what you are seeing in the field.\n",
    "        records = []\n",
    "        i = 0\n",
    "        ''' Important Note! \n",
    "            ebcdic file: Open the file with rb \"read binary\" and f2.read(size)\n",
    "            ascii file : Open the file with  r \"read text\" and f2.readline()\n",
    "        '''\n",
    "        with open(coboldata_full_path, 'r') as f2:\n",
    "            while True:\n",
    "                row = {}\n",
    "                # ASCII\n",
    "                data = f2.readline()\n",
    "                if not data:\n",
    "                    # got eof\n",
    "                    break\n",
    "                \n",
    "                i += 1\n",
    "                row['Registry'] = i\n",
    "                \n",
    "                if data[0:2] == \"02\":         # for ASCII\n",
    "                    parser.parse(data)\n",
    "                    dict_value = parser.value\n",
    "                    \n",
    "                    ## ALERT: Don't use json.dumps. It doesn't threat Decimal formats - float for monetary values\n",
    "                    # Use simplejson instead, it has support for Decimals\n",
    "                    row['data'] = simplejson.dumps(dict_value)\n",
    "                    \n",
    "                    row['status'] = 'success'\n",
    "                else:\n",
    "                    row['status'] = f'failure : Registry type {data[0:2]} not processed'\n",
    "                \n",
    "                records.append(row)\n",
    "                \n",
    "        return records\n",
    "    \n",
    "    # -------------- MAIN ---------------\n",
    "    return udf(p_copybook_def_path ,p_coboldata_path)\n",
    "    # return list_directory(TARGET_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b869ed76-baed-44aa-bdf8-39b5b93db7c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "|\"COBOL_PARSED\"                                      |\n",
      "------------------------------------------------------\n",
      "|[                                                   |\n",
      "|  {                                                 |\n",
      "|    \"Registry\": 1,                                  |\n",
      "|    \"data\": \"{\\\"DATA1-REGISTRY-TYPE\\\": 2, \\\"DAT...  |\n",
      "|    \"status\": \"success\"                             |\n",
      "|  },                                                |\n",
      "|  {                                                 |\n",
      "|    \"Registry\": 2,                                  |\n",
      "|    \"data\": \"{\\\"DATA1-REGISTRY-TYPE\\\": 2, \\\"DAT...  |\n",
      "|    \"status\": \"success\"                             |\n",
      "|  },                                                |\n",
      "|  {                                                 |\n",
      "|    \"Registry\": 3,                                  |\n",
      "|    \"data\": \"{\\\"DATA1-REGISTRY-TYPE\\\": 2, \\\"DAT...  |\n",
      "|    \"status\": \"success\"                             |\n",
      "|  }                                                 |\n",
      "|]                                                   |\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = session.sql('''select \n",
    "    cobolparser_snowpy('ASCII_BOOK.cob' ,'ASCII_DATA.bin') as cobol_parsed\n",
    "''')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14234f7c-c65d-455a-ab7b-fa769777725c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#close the snowpark session\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
